{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b90d207",
   "metadata": {},
   "source": [
    "먼저, 기본적인 딥러닝 구조들을 살펴보자.\n",
    "딥러닝 실행을 위해서는 보통\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "를 import 해와야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972b4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import PIL\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68e299",
   "metadata": {},
   "source": [
    "모델을 돌리기 위해서 data를 load해오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c2890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "transform = transforms.Compose(#전처리, 일반적으로 사용하는 것이 정해져있음\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, ))])#normalize해줌: (0.5,0.5,0.5)로 하면 RGB인거고 MNIST는 gray라서 (0.5,)\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform)\n",
    "\n",
    "# Data Loader \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa3822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./mnist_data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb52ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./mnist_data/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a580cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ./mnist_data/\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(0.5,))\n",
       "            ),\n",
       " 'num_workers': 0,\n",
       " 'prefetch_factor': 2,\n",
       " 'pin_memory': False,\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 32,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x7f4dba0ef810>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7f4dba0efd10>,\n",
       " 'generator': None,\n",
       " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
       " 'persistent_workers': False,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None,\n",
       " '_iterator': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fd679",
   "metadata": {},
   "source": [
    "NN model 정의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d97771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn#weight공유가 가능함\n",
    "import torch.nn.functional as F#단순 연산만 해줌\n",
    "\n",
    "class Network(nn.Module):#torch.nn에 있는 Module을 쓸거다: \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()#super을 통해서 nn.Module을 상속받았어: super는 기반 class의 method를 호출해주는 애\n",
    "        self.dense1 = nn.Linear(784, 512)\n",
    "        self.dense2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, inputs):#내가 쓸 layer을 생성한다: forward를 잘 정의한다\n",
    "        output = inputs.view(-1,784)\n",
    "        output = self.dense1(output)\n",
    "        output = self.dense2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cc74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# use gpu -> .cuda()  -> model and Variable \n",
    "model = Network().cuda()#cuda를 이용하면 GPU로 할당이 가능하다: variable().cuda()로 사용을 한다\n",
    "# This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. so not use softmax()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#손실함수->기본적으로 logsoftmax()가 내장되어있음.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)#model의 parameter를 할당해줌; RMSprop는 이동평균, adagrad나 adam같은애들도 쓸수 있겠지!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5760400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (dense1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (dense2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model#model은 내가 정의한 network: input feature은 MNIST: 28*28 = 784, output; 10개 class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013e9a7",
   "metadata": {},
   "source": [
    "train 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185f6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable # autograd는 자동미분해주는애, \n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for data, label in train_loader:#train_loader: train data를 받아오는 함수로 data, label값을 return해줌\n",
    "        data, label = Variable(data).cuda(), Variable(label).cuda()#cuda에 올려준다: variable class가 tensor를 감싸고 있는\n",
    "        \n",
    "#     for i, datas in enumerate(train_loader, 0):\n",
    "#         data, labels = datas\n",
    "#         data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        output = model(data)#forward를 해서 data를 모델에 전달해서 예상하는 label값을 계산함.\n",
    "        loss = criterion(output, label)#내 모델의 output과 label 사이의 loss를 계산: 앞에서 정의한 criterion을 이용해서,\n",
    "        \n",
    "        optimizer.zero_grad()#갱신할 variable에 대한 변화 다 0으로 만듦: gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문, 즉 iteration 한 번 돌때마다 grad 초기화\n",
    "        \n",
    "        loss.backward() #역전파단계: 모델의 variable에 대한 loss의 변화 계산: 자동으로 모든 grad계산해줌\n",
    "        optimizer.step()# 가중치 갱신: backprop를 통해서 계산된 변화를 이용해 parameter를 갱신한다\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]#최대값의 index return\n",
    "        train_acc += pred.eq(label.data.view_as(pred)).sum()#pred와 data값을 비교해서 같으면 1\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Train Epoch: {} Average loss: {:.4f} Accuracy : {:.4f}%)'.format(epoch, train_loss, 100. * train_acc / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfbb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    for data, target in test_loader:\n",
    "        # volatile=True no use backprob\n",
    "        data, target = Variable(data, volatile=True).cuda(), Variable(target).cuda()\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        test_acc += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {:.0f}%)'.format(test_loss, 100. * test_acc / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fd6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Average loss: 0.0157 Accuracy : 86.5133%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0102, Accuracy: 91%)\n",
      "Train Epoch: 2 Average loss: 0.0103 Accuracy : 90.4917%)\n",
      "Test set: Average loss: 0.0096, Accuracy: 91%)\n",
      "Train Epoch: 3 Average loss: 0.0097 Accuracy : 91.0900%)\n",
      "Test set: Average loss: 0.0091, Accuracy: 92%)\n",
      "Train Epoch: 4 Average loss: 0.0093 Accuracy : 91.4450%)\n",
      "Test set: Average loss: 0.0090, Accuracy: 92%)\n",
      "Train Epoch: 5 Average loss: 0.0092 Accuracy : 91.6183%)\n",
      "Test set: Average loss: 0.0089, Accuracy: 92%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154824a",
   "metadata": {},
   "source": [
    "CNN은 보통 이미지 처리에서 많이 사용하며, \n",
    "앞에서 쓴 dense layer에 비해 시냅스 강도가 약해짐; 필요한애만 쓸 수 있고, 저장해야 할 parameter 수가 줄어듦!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "393f463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequantial은 forward에서 구현할 layer을 더 가독성있게 작성해줄 수 있는 애임, 예를들어서\n",
    "class MyNeuralNetwork(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(MyNeuralNetwork, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5) \n",
    "        self.fc1 = nn.Linear(in_features=30*5*5, out_features=128, bias=True) \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10, bias=True) \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.conv1(x), inplace=True) \n",
    "        x = F.max_pool2d(x, (2, 2)) \n",
    "        x = F.relu(self.conv2(x), inplace=True) \n",
    "        x = F.max_pool2d(x, (2, 2)) \n",
    "        x = x.view(x.shape[0], -1) \n",
    "        x = F.relu(self.fc1(x), inplace=True) \n",
    "        x = F.relu(self.fc2(x), inplace=True) \n",
    "        \n",
    "        return x\n",
    "    \n",
    "#이 class를 layer 단위로 보고 싶으면\n",
    "class MyNeuralNetwork(nn.Module): \n",
    "    \n",
    "    def __init__(self): \n",
    "        super(MyNeuralNetwork, self).__init__() \n",
    "        self.layer1 = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5), \n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(2) ) \n",
    "        self.layer2 = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5), \n",
    "            nn.ReLU(inplace=True), nn.MaxPool2d(2) ) \n",
    "        self.layer3 = nn.Sequential( \n",
    "            nn.Linear(in_features=30*5*5, out_features=128, bias=True), \n",
    "            nn.ReLU(inplace=True) ) \n",
    "        self.layer4 = nn.Sequential( \n",
    "            nn.Linear(in_features=128, out_features=10, bias=True), \n",
    "            nn.ReLU(inplace=True) ) \n",
    "        #이렇게 각 layer을 sequential로 정의를 한 다음에\n",
    "        \n",
    "        def forward(self, x): \n",
    "            x = self.layer1(x) \n",
    "            x = self.layer2(x) \n",
    "            x = x.view(x.shape[0], -1) \n",
    "            x = self.layer3(x) \n",
    "            x = self.layer4(x) \n",
    "            return x\n",
    "#forward에서 layer로 작성을 해줌\n",
    "\n",
    "#출처: https://dororongju.tistory.com/147 [웹 개발 메모장]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
