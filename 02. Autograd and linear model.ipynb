{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eae16e1",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb474873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5acb5c",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779e1e1",
   "metadata": {},
   "source": [
    "tensor 그냥은 require_grad가 False인데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20243455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "x.requires_grad # False\n",
    "y = torch.ones(1)\n",
    "y.requires_grad#False\n",
    "z = x+y \n",
    "z.requires_grad #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbff52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_()\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38838de8",
   "metadata": {},
   "source": [
    "이렇게 True로 바꿔줄 수 있고, True로 바꿔주면 걔를 이용한 연산을 해도 requires grad = True가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531e7c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x + y\n",
    "z.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454aca74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65849adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092bcbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9d4bc",
   "metadata": {},
   "source": [
    "이렇게 requires_grad True로 설정을 해주면, y.sum()이나 y.max를 하면 backward를 알아서 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe1d22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c65c7e",
   "metadata": {},
   "source": [
    "backward가 자동으로 sum에 대한 backward라고 설정이 되고, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214b1888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [7., 9.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 2*y +1\n",
    "z # auto grad는 맨 마지막에 있는 것에 대한 grad, 여기서는 +가 마지막에 있어서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7efdefd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7fcdd14a20d0>, 0), (None, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions#next_function을 통해 다음 backward를 파악할 수 있음. + 다음에 곱하기 하니까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38cc8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = z.sum()\n",
    "out.backward() #z는 행렬이라 backward 안됨, backward는 scalar output만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad2147cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "None\n",
      "None\n",
      "y.grad_fn : None\n",
      "z.grad_fn : <AddBackward0 object at 0x7fcdd14e9950>\n",
      "out.grad_fn : <SumBackward0 object at 0x7fcdd14e9d10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  \n",
      "/home/tako/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(out.grad) # 메모리 때문에 grad function 없는 마지막에 선언한것만 저장\n",
    "print(\"y.grad_fn :\", y.grad_fn)\n",
    "print(\"z.grad_fn :\", z.grad_fn)\n",
    "print(\"out.grad_fn :\", out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b8f8e",
   "metadata": {},
   "source": [
    "tensor을 numpy로 바꾸고 싶을때,\n",
    "z.numpy를 하면 z에는 grad 등이 저장되어 있기 때문에 바로 변환할 수 없음.\n",
    "따라서, z.detach().numpy() 혹은 z.data().numpy()를 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d19222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 5.],\n",
       "       [7., 9.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "139c0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7183,  7.3891, 20.0855], grad_fn=<ExpBackward>)\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0.], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "b = a.exp()\n",
    "print(b)\n",
    "# c는 b를 graph에서 떼어내어 require gradient = False\n",
    "# 허나, 원본 데이터는 공유\n",
    "c = b.detach()\n",
    "c.zero_()\n",
    "print(c)\n",
    "print(b)# b도 초기화가 됨.\n",
    "#여기서 b.sum().backward()을 하면 b가 초기화 되었기 때문에 에러가 남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b570799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)# backward가 안되었기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85e86f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7183,  7.3891, 20.0855], grad_fn=<ExpBackward>)\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0.], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "b = a.exp()\n",
    "print(b)\n",
    "\n",
    "# 허나, 원본 데이터는 공유\n",
    "c = b.data\n",
    "c.zero_()\n",
    "print(c)\n",
    "print(b)# b도 초기화가 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59706839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum().backward() #data는 grad에 영향을 주지 않기 때문에 inplace가 일어나도 에러가 뜨지 않음.\n",
    "a.grad#따라서 잘못된 결과가 나왔음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d19801",
   "metadata": {},
   "source": [
    "# Linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f23c",
   "metadata": {},
   "source": [
    "## 선형회귀와 신경망\n",
    "1. Forward 단계.\n",
    "데이터 처리 -> 모델 구현 -> 예측값 도출 -> loss function 계산\n",
    "2. Backward.\n",
    "기울기 계산 -> 개선 방향 찾기 -> 가중치 개선(w를 바꿔줌)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d194b4",
   "metadata": {},
   "source": [
    "- 신경망은 input layer, hidden layer, output layer로 구성되어 있다.\n",
    "- 선형신경망에서, hidden layer의 개수를 늘려도 선형결합이기 때문에 layer를 하나만 사용한 것과 같은 효과가 나온다.\n",
    "- 이를 방지하기 위해 activation function을 추가해 선형성을 깨트릴 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f689575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91787fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=228, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(228, 1, bias=True) #linear model을 만들 때, input, outputm bias 를 설정해준다.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbb23a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5) # linear model에서 사용할 loss와 optimizer을 결정해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(50000) :\n",
    "    pre = model(x)\n",
    "    cost = loss(pre, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 5000 == 0 :\n",
    "        print(\"Step : \",step, \", Cost : \", cost.item()) #보통 이런식으로 training을 함, 지금은 data를 load해오지 않아서 돌아가지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab33b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
